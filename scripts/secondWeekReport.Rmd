---
title: $\color{#E31A1C}{\text{Belkin}}$ vs. $\color{#6A3D9A}{\text{Elago}}$
subtitle: 'Train the models'
author: "Murilo Miranda"
date: "`r format(Sys.Date(), '%Y-%B-%d')`"
output:
  html_document:
    toc: true # table of content true
    toc_float: true
    code_folding: hide
    toc_depth: 2  # upto three depths of headings (specified by #, ## and ###)
    theme: united
    highlight: breezedark  # specifies the syntax highlighting style
    #css: styles.css
  pdf_document: default
version: '0.3'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE)

library(summarytools)
st_options(plain.ascii   = FALSE,       # One of the essential settings
           style         = "rmarkdown", # Idem.
           dfSummary.silent  = TRUE,# Suppresses messages about temporary files
           footnote      = NA,          # Keeping the results minimalistic
           subtitle.emphasis = FALSE)   # For the vignette theme, this gives
                                            # much better results. Your mileage may vary.

st_css()
```

```{r, message = FALSE}
library(here)
library(kableExtra)
library(sjPlot)
library(forcats)
library(dplyr)
library(ggplot2)
library(RColorBrewer)
library(doParallel)
library(caret)
```


```{r, message = FALSE, echo=FALSE}
colors <- brewer.pal(12, "Paired")
```

# Loading data
The data has empty values and the columns are separated by semicolon. **read.csv2** function was used because it has semi-colon as default sep-argument. To solve the problem with empty values, na.strings argument was used with empty value and single space.

```{r, echo=FALSE}
data <- read.csv2(here("data/surveyBelkinElago.csv"), na.strings = c("", " "))

str(data)
```

# Preprocessing
### Check data types
In the data-set elevel, car and zipcode are integers. In reality, they are categorical variables, so they are converted into factor. The missing values are converted into 'No Response'.

### Data conversions
```{r, echo=FALSE}
data$elevel <- as.factor(data$elevel)
levels(data$elevel) <- c("HSch", "Prof", "Coll", "Mast")
data$elevel <- reorder(data$elevel, data$elevel, FUN = function(x) -length(x))

data$car <- as.factor(data$car)
levels(data$car) <- c("BMW", "Tesl", "Volk", "Fiat", "Chry", "Cit1", "Ford", "Hond", "Opel", "Cit2", "Pors", "Peug", "Audi", "Merc", "Kia", "Niss", "Hyun", "Rena", "Toyo", "Daci")
data$car <- reorder(data$car, data$car, FUN = function(x) -length(x))

data$zipcode <- as.factor(data$zipcode)
levels(data$zipcode) <- c("C", "EC", "E", "NE", "N", "NW", "SE", "SW", "W")
data$zipcode <- reorder(data$zipcode, data$zipcode, FUN = function(x) -length(x))

# convert NA to 'No Response' in the brand variable
levels(data$brand) <- c(levels(data$brand), "No Response")
data$brand[is.na(data$brand)] <- "No Response" 
data$brand <- factor(data$brand, levels = c("No Response", "Belkin", "Elago"))

head(data)
```

### Check duplicate observations
```{r, echo=FALSE}
anyDuplicated(data)
```

### Separate the dataset into two dataset
dataSet with all information that will be used to train our models and no Response dataset as new information and we will use it to predict if those customers will prefere Belkin or Elago.

```{r, echo=FALSE}
dataSet <- data %>% filter(brand != "No Response") %>% droplevels()
noResponse <- data %>% filter(brand == "No Response") %>% droplevels()
```

### Correlation
```{r, echo=FALSE}
cor(dataSet[, c("salary","age","credit")])
```

## Distributions of the variables

```{r, echo=FALSE, results='asis'}
dfSummary(dataSet, plain.ascii = FALSE, style = "grid", 
          graph.magnif = 0.75, valid.col = FALSE, tmp.img.dir = "/tmp")
```

# Exploratory Data Analysis

```{r, echo=FALSE}
hgram <- ggplot(dataSet, aes(salary, fill = brand)) + geom_histogram(bins = 100) +
  scale_fill_manual(values = c(colors[6], colors[10])) +
  geom_vline(aes(xintercept = median(data$salary)), linetype = "dashed") +
  labs(
    x = "",
    y = "",
    title = "Salary distribution by brand preference",
    subtitle = "Salaries are expressed in yearly thousands of euros",
    caption = "Source: Ubiqum 2020 (Fake Data)"
  ) +
  geom_text(x = 80000, y = 70, label = "Belkin", color = "white") +
  geom_text(x = 40000, y = 30, label = "Elago", color = "white")
 
hgram + theme(panel.background = element_rect(fill = "white", colour = "white"),
              plot.title = element_text(hjust = -0.15),
              plot.subtitle = element_text(hjust = -0.17),
              legend.position = "none")
```

```{r, echo=FALSE}
hgram <- ggplot(dataSet, aes(age, fill = brand)) + geom_histogram(bins = 31) +
  scale_fill_manual(values = c(colors[6], colors[10])) +
  geom_vline(aes(xintercept = median(dataSet$age)), linetype = "dashed") +
  labs(
    x = "",
    y = "",
    title = "Age distribution by brand preference",
    subtitle = " ",
    caption = "Source: Ubiqum 2020 (Fake Data)"
  ) +
  geom_text(x = 65, y = 220, label = "Belkin", color = "white") +
  geom_text(x = 30, y = 100, label = "Elago", color = "white")
 
hgram + theme(panel.background = element_rect(fill = "white", colour = "white"),
              plot.title = element_text(hjust = -0.15),
              plot.subtitle = element_text(hjust = -0.17),
              legend.position = "none")
```

### Model Hypothesis Testing

The null hypothesis ($H_0$): age and educational level don't affect salary, and the alternative hypothesis ($H_1$): age or educational level  do affect the salary amount.

```{r, echo=FALSE}
model <- lm(salary ~ age + elevel, data = dataSet)

tab_model(
  model,
  pred.labels = c("Intercept", "Age", "College Degree", "High School Degree", "Master's Degree"),
  string.ci = "Conf. Int (95%)",
  string.p = "P-Value"
)
```

<br>
```{r, echo=FALSE}
global_labeller <- labeller(
  elevel = c(`HSch` = "< High School\nDegree", `Prof` = "Professional\nDiploma", `Coll` = "College\nDegree", `Mast` = ">= Master's\nDegree"),
  brand = c(`Belkin` = "", `Elago` = "")
)

dataSet %>%
  mutate(elevel = fct_reorder(elevel, salary, .fun='median')) %>%
  ggplot(aes(x = age, y = salary, color = brand)) + geom_point() + 
  scale_color_manual(values = c(colors[6], colors[10])) + 
  scale_x_continuous(limits = c(20, 80), breaks = c(20, 50, 80), expand = c(0.1, 0.1)) +
  labs(
    x = "",
    y = "",
    title = "Salary distribution by educational level and age separated by brand preference",
    subtitle = "High educational level helps to earn more, age doesn't",
    caption = "Source: Ubiqum 2020 (Fake Data)"
  ) +
  facet_grid(. ~ brand + elevel, labeller = global_labeller) + 
  theme(panel.background = element_rect(fill = "white", colour = "white"),
        strip.background = element_blank(),
        plot.title = element_text(hjust = -15.),
        plot.subtitle = element_text(hjust = -.25),
        legend.position = "none")
```

### Conclusion

Age and salary are uniformly distributed and credit is normally distributed. Salary and credit have a high correlation ($\rho$ = 0.795). They are also correlated with educational level, that is, people with a high level of education receive high salary and credit in banks and people with a low level of education receive low salary and credit.

There are more people with **Professional Diploma**, followed by **College Degree**, **High School Degree or lower**, finnally, **Master's Degree or Higher**. Age at educational level is equally distributed.

Older people and people with intermediate income are associated with the $\color{#E31A1C}{\text{Belkin}}$ brand and younger people and people with low or high income are associated with the $\color{#6A3D9A}{\text{Elago}}$ brand.


# Preprocessing 2

Some variables have different units and variance, so we will scale and center the numerical data by using the convenient preprocess() in **caret**. We will also remove variables that only have a single unique value (i.e. a “zero-variance predictor”) and have high correlation. For many models (excluding tree-based models), this may cause the model to crash or the fit to be unstable.

```{r}
# Centering and scaling numerical columns
preProcValues <- preProcess(dataSet, method = c("center","scale", "zv", "corr"))

dataSetProcessed <- predict(preProcValues, dataSet)
```

### Splitting data using caret
We will use createDataPartition() to split our data into two sets: 75% in the training set and 25% in the testing set.

```{r}
# Spliting training set into two parts based on outcome: 75% and 25%
index <- createDataPartition(dataSetProcessed$brand, p = 0.75, list = FALSE)
trainSet <- dataSetProcessed[ index,]
testSet <- dataSetProcessed[-index,]
```

## Training models
### Train Control
10-fold cross validation with one repetition. 

```{r}
fitControl <- trainControl(method = "repeatedcv", number = 10, repeats = 1, 
                           classProbs = TRUE, summaryFunction = twoClassSummary)
```

## Decision Tree
Build a model using a decision tree, C5.0,  on the training set with 10-fold cross validation and an Automatic Tuning Grid with a tuneLength of 2. 

```{r}
# set.seed(123)
# cl <- makeCluster(2)
# registerDoParallel(cl)
# dtFit <- train(brand ~ ., trainSet, method = "C5.0", trControl = fitControl, 
#                tuneLength = 2, metric = "ROC")
# stopCluster(cl)
# saveRDS(dtFit, "decisionTreeC5.RDS")
dtFit <- readRDS("decisionTreeC5.RDS")
#summary(dtFit)
#ggplot(dtFit)
```

```{r}
dtBrand <- predict(dtFit, newdata = testSet)
#dtProbs <- predict(dtFit, newdata = testSet, type = "prob")

confusionMatrix(dtBrand, testSet$brand)
```

## Random Forest
Build a decision tree using Random Forest with 10-fold cross validation and manually tune 5 different mtry values.

### Parameter Tuning
Manually tune 5 different mtry values (2, 3, 4, 5, 6).

```{r}
rfGrid <- expand.grid(mtry = c(2, 3, 4, 5, 6))
```
```{r}
# set.seed(123)
# cl <- makeCluster(2)
# registerDoParallel(cl)
# rfFit <- train(brand ~ ., trainSet, method = "rf", trControl = fitControl, 
#                tuneGrid = rfGrid, metric = "ROC")
# stopCluster(cl)
# saveRDS(rfFit, "randomForest.RDS")
rfFit <- readRDS("randomForest.RDS")
#rfFit
#ggplot(rfFit)
```

```{r}
rfBrand <- predict(rfFit, newdata = testSet)
#rfProbs <- predict(rfFit, newdata = testSet, type = "prob")

confusionMatrix(rfBrand, testSet$brand)
```

### Variable Importance
```{r}
par(mfrow=c(1,2))

plot(varImp(dtFit), main = "C5.0")
plot(varImp(rfFit), main = "Random Forest")
```

## Comparing models
```{r}
resamps <- resamples((list(dt = dtFit, rf = rfFit)))

summary(resamps)
#xyplot(resamps,what = "BlandAltman")

diffs <- diff(resamps)
summary(diffs)
```

# Combining all surveys (Complete and Incomplete)

## Predict brand variable for incomplete survey
ROC shows that decision tree is better than random forest (diff = 0.006, p < 0.05), but sensitivity and specificity are no statistically different (p = 0.290 and p = 0.106, respectively). Therefore, we use decision tree to predict the missing information in the survey.

```{r}
noRespProcessed <- predict(preProcValues, noResponse)
newBrand <- predict(dtFit, newdata = noRespProcessed)

newBrand <- cbind(noResponse[, -7], brand = newBrand)

newDataSet <- rbind(dataSet, newBrand)
```

### Distributions of the variables

```{r, echo=FALSE, results='asis'}
dfSummary(newDataSet, plain.ascii = FALSE, style = "grid", 
          graph.magnif = 0.75, valid.col = FALSE, tmp.img.dir = "/tmp")
```

# Exploratory Data Analysis

```{r, echo=FALSE}
hgram <- ggplot(newDataSet, aes(salary, fill = brand)) + geom_histogram(bins = 100) +
  scale_fill_manual(values = c(colors[6], colors[10])) +
  geom_vline(aes(xintercept = median(data$salary)), linetype = "dashed") +
  labs(
    x = "",
    y = "",
    title = "Salary distribution by brand preference",
    subtitle = "Salaries are expressed in yearly thousands of euros",
    caption = "Source: Ubiqum 2020 (Fake Data)"
  ) +
  geom_text(x = 80000, y = 100, label = "Belkin", color = "white") +
  geom_text(x = 40000, y = 50, label = "Elago", color = "white")
 
hgram + theme(panel.background = element_rect(fill = "white", colour = "white"),
              plot.title = element_text(hjust = -0.15),
              plot.subtitle = element_text(hjust = -0.17),
              legend.position = "none")
```

```{r, echo=FALSE}
hgram <- ggplot(newDataSet, aes(age, fill = brand)) + geom_histogram(bins = 31) +
  scale_fill_manual(values = c(colors[6], colors[10])) +
  geom_vline(aes(xintercept = median(dataSet$age)), linetype = "dashed") +
  labs(
    x = "",
    y = "",
    title = "Age distribution by brand preference",
    subtitle = " ",
    caption = "Source: Ubiqum 2020 (Fake Data)"
  ) +
  geom_text(x = 65, y = 300, label = "Belkin", color = "white") +
  geom_text(x = 40, y = 200, label = "Elago", color = "white")
 
hgram + theme(panel.background = element_rect(fill = "white", colour = "white"),
              plot.title = element_text(hjust = -0.15),
              plot.subtitle = element_text(hjust = -0.17),
              legend.position = "none")
```

### Model Hypothesis Testing

The null hypothesis ($H_0$): Salary of customers are equal between brand, and the alternative hypothesis ($H_1$): Salary of customers are diferent between brand.

```{r, echo=FALSE}
model <- t.test(salary ~ brand, data = newDataSet)

model
```

In the result above: $t$ is the Student $t$-test statistics value ($t$ = -1.635), df is the degrees of freedom (df= 14200), p-value is the significance level of the $t$-test (p = 0.102). The confidence interval (conf.int) of the mean differences at 95% is also shown (conf.int= [-2207.222, 199.832]); and finally, we have the means of the two groups of samples (average salary of customers who perfer Belkin = 86716.46, average salary of customers who perfer Elago = 87720.15). The p-value of the test is greater than 0.05. We can then accept the null hypothesis and conclude that people who prefer Elago earn the same amount of money than people who prefer Belkin.


```{r,echo=FALSE}
hgram <- ggplot(newDataSet, aes(brand, salary, fill = brand)) + geom_boxplot() +
  scale_fill_manual(values = c(colors[6], colors[10])) +
  labs(
    x = "",
    y = "",
    title = "Salary distribution by brand preference",
    subtitle = " ",
    caption = "Source: Ubiqum 2020 (Fake Data)"
  )
#+
#  geom_text(x = 65, y = 100, label = "Belkin", color = "white") +
#  geom_text(x = 30, y = 50, label = "Elago", color = "white")
 
hgram + theme(panel.background = element_rect(fill = "white", colour = "white"),
              plot.title = element_text(hjust = -0.15),
              plot.subtitle = element_text(hjust = -0.17),
              legend.position = "none")
```

### Conclusion

There is no big difference between models. On average, the ROC for decision tree is 0.859 and for Random forest is 0.853 (p = 0.009). Decision tree C5.0 shows that salary, age, some car models (BMW and Citroen) and Northeastern Europe region are the most important variables. Random forest also shows that salary and age are the most important variables.

We decided to use the decision tree C5.0 to predict the missing information of preferences of customers because this method seems more parsimonious as it only uses three variables (salary, age and car model), while the other uses all variables.

In conclusion, the customers prefer equally Elago and Belkin, although the customers who prefer Elago earn on average a bit more (\$87720 $\pm$ \$42973) than Belkin (\$86716 $\pm$ \$31685).

